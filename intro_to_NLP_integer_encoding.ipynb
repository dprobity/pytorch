{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXpHdH/KSYpx3fjNgDPTQP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dprobity/pytorch/blob/main/intro_to_NLP_integer_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets experiement with sentiment analysis"
      ],
      "metadata": {
        "id": "IMpCEDJGEvlZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wnLijJ1IEGf2"
      },
      "outputs": [],
      "source": [
        "\n",
        "  # we need to do tokenization\n",
        "\n",
        "  # we use torch.nn.utils.rnn.pad_sequence() will be used to pad so each tensor have same length or dimension\n",
        "  # we build a word dictionary by adding all the words in a sentence to a set() and we return only unique words and then sort it and tokenize\n",
        "  # by assigning unique numbers to each words and then use 0 for padding\n",
        "\n",
        "\n",
        "\n",
        "from typing import List\n",
        "import numpy as np\n",
        "\n",
        "class SentimentDatasetBuilder:\n",
        "\n",
        "    def get_dataset(self, positive: List[str], negative: List[str]) -> np.ndarray:\n",
        "        # Build vocabulary\n",
        "        vocabulary = set()\n",
        "        for sentence in positive:\n",
        "            for word in sentence.split():\n",
        "                vocabulary.add(word.lower())\n",
        "\n",
        "        for sentence in negative:\n",
        "            for word in sentence.split():\n",
        "                vocabulary.add(word.lower())\n",
        "\n",
        "        vocabulary = sorted(list(vocabulary))\n",
        "        word_index = {word: i for i, word in enumerate(vocabulary)}\n",
        "\n",
        "        # Vectorize sentences\n",
        "        def vectorize(sentence: str) -> np.ndarray:\n",
        "            vec = np.zeros(len(vocabulary))\n",
        "            for word in sentence.split():\n",
        "                w = word.lower()\n",
        "                if w in word_index:\n",
        "                    vec[word_index[w]] += 1\n",
        "            return vec\n",
        "\n",
        "        dataset = []\n",
        "        labels = []\n",
        "\n",
        "        for s in positive:\n",
        "            dataset.append(vectorize(s))\n",
        "            labels.append(1)\n",
        "\n",
        "        for s in negative:\n",
        "            dataset.append(vectorize(s))\n",
        "            labels.append(0)\n",
        "\n",
        "        dataset = np.array(dataset)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        return dataset, labels, vocabulary\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bGCQZYj-fd2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"jp797498e/twitter-entity-sentiment-analysis\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHaYOqJ1fdrv",
        "outputId": "b8faf06a-871b-4d5e-afc8-89e5905dfe31"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jp797498e/twitter-entity-sentiment-analysis?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.99M/1.99M [00:00<00:00, 3.42MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/jp797498e/twitter-entity-sentiment-analysis/versions/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "positive = [\n",
        "    \"I love this movie\",\n",
        "    \"This product is amazing\"\n",
        "]\n",
        "\n",
        "negative = [\n",
        "    \"I hate this movie\",\n",
        "    \"This product is terrible\"\n",
        "]\n",
        "\n",
        "dataset = SentimentBoWDataset(positive, negative)\n",
        "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "for batch_x, batch_y in loader:\n",
        "    print(batch_x)\n",
        "    print(batch_y)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "I6kmL7k9H81-",
        "outputId": "2b54401a-448f-4262-9365-0182e75b3803"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'SentimentBoWDataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3765424167.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m ]\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentimentBoWDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SentimentBoWDataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#!pip install torchtyping\n",
        "from torchtyping import TensorType\n",
        "\n",
        "positive=[\"Good case, Excellent value.\",\"Great for the jawbone.\",\"The mic is great.\",\"If you are Razr owner...you must have this!\",\"Highly recommend for any one who has a blue tooth phone\"]\n",
        "negative=[\"So there is no way for me to plug it in here in the US unless I go buy a converter.\",\"Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!\",\"I have to jiggle the plug to get it to line up right to get decent volume.\",\"Needless to say, I wasted my money.\",\"What a waste of money and time!\"]\n",
        "\n",
        "\n",
        "\n",
        "# torch.tensor(python_list) returns a Python list as a tensor\n",
        "class Solution:\n",
        "  def get_dataset(self, positive: List[str], negative: List[str]) -> TensorType[float]:\n",
        "    vocabulary = set()\n",
        "    for sentence in positive:\n",
        "      for word in sentence.split():\n",
        "            vocabulary.add(word)\n",
        "\n",
        "    for sentence in negative:\n",
        "      for word in sentence.split():\n",
        "            vocabulary.add(word)\n",
        "\n",
        "    sorted_list = sorted(list(vocabulary))\n",
        "    word_to_int = {}\n",
        "    for i in range(len(sorted_list)):\n",
        "      word_to_int[sorted_list[i]] = i +1\n",
        "\n",
        "    tensors = []\n",
        "    for sentence in positive:\n",
        "      curr_list = []\n",
        "      for word in sentence.split():\n",
        "        curr_list.append(word_to_int[word])\n",
        "      tensors.append(torch.tensor(curr_list))\n",
        "\n",
        "    for sentence in negative:\n",
        "      curr_list = []\n",
        "      for word in sentence.split():\n",
        "        curr_list.append(word_to_int[word])\n",
        "      tensors.append(torch.tensor(curr_list))\n",
        "\n",
        "    return nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
        "\n",
        "\n",
        "\n",
        "gen_tensor = Solution().get_dataset(positive, negative)\n",
        "print(gen_tensor)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJYlX7-WHUxO",
        "outputId": "5141e91d-29b9-48bc-97b3-a46d56d7239e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 3, 22,  2, 67,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0],\n",
            "        [ 4, 27, 59, 37,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0],\n",
            "        [12, 42, 35, 30,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0],\n",
            "        [ 7, 73, 19, 10, 52, 47, 32, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0],\n",
            "        [ 5, 55, 27, 18, 51, 72, 31, 16, 20, 64, 53,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0],\n",
            "        [11, 60, 35, 49, 71, 27, 41, 63, 54, 36, 34, 33, 34, 59, 14, 65,  6, 29,\n",
            "         21, 16, 25],\n",
            "        [13, 63, 23, 27, 24, 39, 46, 58,  1, 43,  9,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0],\n",
            "        [ 6, 32, 63, 38, 59, 54, 63, 28, 36, 63, 40, 66, 56, 63, 28, 26, 68,  0,\n",
            "          0,  0,  0],\n",
            "        [ 8, 63, 57,  6, 70, 48, 45,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0],\n",
            "        [15, 16, 69, 50, 44, 17, 62,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "grw_SaQYJN5W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}